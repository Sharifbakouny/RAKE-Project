%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{array}
\usepackage{makecell}
\usepackage{url}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{titlesec}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

\setlength\titlebox{3.8cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CMPUT 497 Project Report: \\ RAKE - Key Word Extraction Replication}

\author{Shouyang Zhou \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt shouyang@ualberta.ca} \\\And
  Sharon Hains \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt hains@ualberta.ca} \\\And
  Sharif Bakouny\\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt albakoun@ualberta.ca} \\}
  


\date{}

\begin{document}
\maketitle

\begin{abstract}
We attempt to replicate an evaluation from a paper titled  “Automatic Keyword Extraction from Individual Documents”, which compares two keyword extraction methods, RAKE and TextRank, on human labeled scientific abstracts. We cannot replicate that RAKE, using a tailored stop-list, outperforms TextRank. However, we note difficulties replicating the critical tailoring method as described by authors of RAKE. Our experiments indicate that RAKE performs marginally better using a generalized stop-list. 	
\end{abstract}

\section{Introduction}

We replicate the primary evaluation from "Automatic Keyword Extraction from Individual Documents" by \citet{1}. This paper devises an unsupervised method for keyword extraction titled Rapid Automatic Keyword Extraction (RAKE). It is compared to a contender unsupervised method called "TextRank" comparing performance on keyword extraction on a data-set of human labeled scientific abstracts. Both methods have similar approaches and are inspired by the same seminal work. RAKE constructs an adjacency matrix to score candidate keywords. TextRank constructs a weighted graph of keywords and scores them based on a recursive voting scheme treating edges as votes to/from candidate keywords.

Keyword extraction is the automated process of extracting important words and phrases from a document. From an input of text, keyword extraction generates an output of a set of key terms. The importance of keyword extraction is in application, in information retrieval, feature engineering, and assist human labeling tasks. Keyword extraction helps humans process unstructured data in a more efficient and digestible manner. There is widespread discussion on the application of both methods on the web however few attempts to replicate the originating work. It is important to verify the performance of both methods given the amount of discussion.  

Additionally, we replicate a stop-list generation method \citet{1} suggest that complements the unsupervised method RAKE. The input is a corpus and associated human annotated keywords. The output is a list of stop-words which RAKE uses to pre-process texts. 

In summary, we cannot replicate \citet{1}'s salient conclusion, that RAKE using a domain specific generated stop-list, is superior to TextRank and baseline results. We acknowledge difficulties in replicating the stop-list generation method the authors suggest which is crucial to RAKE's functionality. We are able conclude that RAKE, using a generic stop-list, and TextRank perform similarly albeit RAKE has a slight improvement in performance. We replicate Table 1.2 in \citet{1}, in our evaluation. This compares the performance of RAKE and TextRank variants (parameters sets) listing the metrics: extracted keywords (total, mean), correct keywords (total, mean), precision, recall, f-measure summarizing the replication-evaluation. 

\section{Related Work}

\subsection{TextRank}

\citet{4} propose TextRank, an unsupervised graph-based keyword extraction algorithm, based on PageRank. Their method precedes RAKE, they claim better performance compared to an older seminal work of \citet{hulth-2003-improved}. This method generates a graph representing text units, (terms, phrases, or sentences) and generates an importance score per unit. The importance score of a vertex is decided by considering global relatedness of each text unit by recursively computing relations (edge weights) from the entire graph. Each text unit's importance is the sum of its baseline weight and a portion of its neighboring text units'. Intuitively, keywords are those whom recursively have the highest relatedness to other terms in the text, the top scoring units are keywords. TextRank is quite flexible and open to extension, the authors supply a default set of parameters. Implementing this algorithm goes as follows:

\begin{enumerate}
\small \itemsep0em 

\item Filter for candidate text units and add them as vertices to a graph. (terms, phrases, or sentences)
\item Generate edges of the graph via some relation between text units. (Authors suggest co-occurrence)
\item Compute a recursive importance score of a vertex as some innate value plus its descendants scores.    
\item Iterate importance scoring until convergence using a damping factor to control updates.
\item Rank vertices based on their final scores and select top T vertices. (Authors suggest using 1/3 of all vertices.)
\item Post-process keywords into key phrases by their adjacency in the text.

\normalsize
\end{enumerate}   

\subsection{Hulth}

Both RAKE and TextRank authors conduct their evaluations by extending the methods of \citet{hulth-2003-improved}. \citet{1} noted difficulty in finding training materials used by Hulth. Hence we not be replicating the results of Hulth. RAKE, TextRank, and Hulth form a linage of keyword extraction methods, each successor claims performance over the others over the same data-set. \citet{1} describe Hulth's method, "Hulth (2003) compares the effectiveness of three term selection approaches: noun-phrase (NP) chunks, n-grams, and POS tags, with four discriminating features of these terms as inputs for automatic keyword extraction using a supervised machine-learning algorithm." Hulth provides the data-set all three authors use to evaluate their keyword extraction methods. This is a data-set of human keyword annotated scientific paper abstracts divided into a training, dev, and test sets. As RAKE and TextRank are unsupervised methods, ignoring stop-list generation for RAKE, both authors focus on using the test set. 

\section{Methodology}

“Automatic Keyword Extraction from Individual Documents” by \citet{1} is the primary article for this replication. As mentioned, they evaluate their method RAKE and compare it with TextRank on human keyword annotated scientific abstracts. RAKE generates an adjacency matrix tracking co-occurrence of text, and define an importance score as the degree of a term divided by the frequency of the term. RAKE requires a list of stop-words. Stop words are punctuation, numbers, conjunctions, and user specified terms which are used to delimit candidate keywords. A brief summary of the RAKE algorithm:

\begin{enumerate}
	\small \itemsep0em 

	\item Split the text into an array of words using the delimiters.
	\item Split the array into sequences contiguous words using stop words and phrase delimiters.
	\item Candidate keywords are words in a sequence that are assigned the same position in the text. 
	\item Assign scores to each keyword candidate using ratio of degree to word-frequency. 
	\item Keywords that contain stop words:
	\begin{enumerate}
		\item A pair of candidate keywords must be adjoined at least twice in the text in the same order.
		\item Create a new keyword which contains the pair of keywords with interior stop words between them.
		\item The new keyword’s score is the sum of the scores of its keywords components.
	\end{enumerate}
	\item The keywords of the text are the top T keywords from the keyword candidates list. (Authors suggest using a size of 1/3 of the adjacency matrix.)

	\normalsize
\end{enumerate}

To complement RAKE the authors also develop a method for stop-list generation they call "Keyword Adjacency" stop-list generation. While RAKE is an unsupervised method the authors suggest that if training data is available it can be beneficial to create a domain specific stop-list.  A brief summary of the stop-list generation algorithm:

\begin{enumerate}
	\small \itemsep0em 

	\item Given training keywords, find each keyword in the abstract.
	\item Look for the adjacent tokens of each keyword. Count these words as 'adjacent words'.
	\item Iterate through the adjacent words list.
	\item For each abstract, find the count of each adjacency word, called the adjacency frequency.
	\item For each keyword, find the count of each adjacency word, called the keyword frequency.
	\item If the keyword frequency is higher than the adjacency frequency, remove this word from the adjacent words list.
	\item All items remaining in the list is our stop-list. 

	\normalsize
\end{enumerate}

\section{Implementation}
We will replicate \citeapos{1} evaluation of the two using third party libraries. We will implement an evaluation script that feeds a data-set into these third party libraries to extract then aggregate the resultant metrics. This task involves data collection, reviewing the third party implementations, understanding the interface to the RAKE and TextRank implementations, pre-processing data-sets to be piped into the two methods, and analyzing the results.

The data from Hulth is sourced from \citet{Boudin}, this GitHub repo is hosted by an associate professor at the University of Nantes researching NLP and information retrieval. The dataset required a Python interface to reproduce the initial unprocessed text and human annotated keywords. The abstracts were stored in XML where each abstract is decomposed into individual tokens-tags groups where each token had been post-processed with its POS tag, its stem, and various other qualities. Some effort was required to reconstruct the initial text.

We conducted a review of candidate third party implementations of RAKE and TextRank, checking that the implementations appears to follow the method descriptions as best to our ability. The authors of RAKE and TextRank give unit examples of input and output which where used to establish functionality. We focus on testing individual steps of the implementations on small examples and the author's examples. Special care was taken on examining the role of tuning parameters, some implementations did not expose an interface for the stated parameter tuning in the papers. In these cases, modifications were taken to expose these parameters. For example, in reviewing PyTextRank, we verify that it constructs an undirected graph which is used in the network's function PageRank, which is an information retrieval method that is the basis of TextRank, and establish their tuning of the PageRank settings such as a convergence coefficient accordingly to the paper. We note that PyTextRank does not expose the window parameter used to establish word co-occurrence relations, so we had to modify the library to expose this parameter.

We chose "PyTextRank" (\citet{PyTextRank}, as our representative implementation of TextRank given its strong similarity to the author's unit examples and its inclusion of the keyphrase postprocessing. The authors of PyTextRank note minor enhancements they make over the author's implementation. PyTextRank leverages spaCy to lemmatize, chunk nouns, and conduct named entity recognition. These improvements serve to prune the graph generated in TextRank for better functionality. RAKE on the otherhand had fewer but common well-functioning implementations (\citet{2}) that conformed to the examples given by the originating paper. 

Lastly, we implemented the keyword adjacency stoplist generation method as described in \citet{1} and generated a stop-list from the Hulth training dataset. \citet{1} does not discuss how the keywords were matched in the abstract, only that they were matched and continued on with the stop-list generation algorithm. We used spaCy's PhraseMatcher to find keyword matches in the abstracts, but on individual abstract evaluation, we found many of the keywords had slightly different wording than what was in the abstract, or did not exist. This rendered PhraseMatcher unable to find the keyword in the abstract. We can infer that during \citet{hulth-2003-improved}'s manual process of keyword selection, keywords were chosen that best represented the abstract's subject.

\section{Evaluation}

As our project is based on replicating the results of a paper, we evaluate the replication aggregate measures recorded as per the initial study and conduct an error analysis from samples from the replicating evaluation. First, we summarize the evaluation from \citeapos{1}, and describe the data-sets and code we use. Then we present our findings and conduct an error analysis of the results. 

RAKE was compared to TextRank and seminal supervised learning methods \citep{hulth-2003-improved} over a data-set of human keyword annotated scientific paper abstracts originating from \citet{hulth-2003-improved}. The authors compare two RAKE variants and two TextRank variants. As both RAKE and TextRank are unsupervised methods they evaluate both methods on Hulth's test set, they largely ignore the test set. One variant of RAKE used a generic stop-list whereas the other variant used a stop-list generated from the training data-set using their stop-list generation method. RAKE was found to outperform all previously used keyword extraction algorithms in precision, efficiency and F-score when using a domain specific stop-list. Using a generic stop-list, RAKE was found to be no worse performing than TextRank.

\subsection{Data Sets \& Code Used}
To summarize, we have gathered or recreated the following data-sets and libraries:

\begin{enumerate}
	\small \itemsep0em 

	\item \citet{hulth-2003-improved}'s data-set of human keyword annotated scientific paper abstracts. After postprocessing, this amounts to the test dataset-subset, and the human annotated keywords per abstract.
	\item A representative implementation of RAKE via NLTK. \citet{2}
	\item A representative implementation of TextRank via NetworkX. \citet{PyTextRank}
	\item Fox's Stoplist, a generic stop-list used in one RAKE variant.
	\item A generated keyword adjacency stop-list, a domain specific stop-list used in one RAKE variant.	
	\item Various candidate implementations of TextRank. (\citet{TextRank-alt1}, \citet{TextRank-alt2}, \citet{TextRank-alt3})

	\normalsize
\end{enumerate}  

We will be generating the Keyword Adjacency stop-list, which will be reproduced using the algorithm described in \citet{1}. To complete this, we tokenize both each abstract and its keywords with spaCy's PhraseMatcher, and then completing the stop-list generation algorithm with this tokenized text. We will then test RAKE with our generated stop-list. 

\subsection{Evaluation Metrics}

The measures reported in the original study by method were: number of extracted keywords, correct number of extracted keywords, average precision, average recall, and average f-measure (f1-score).  We will compare our recorded measures to that of the original study to what extent are \citeapos{1} results reproducible. We will consider the results reproducible if the ordinal performance between RAKE and TextRank variants can be verified. 

\subsection{Results}

We present our replication in Table \ref{table:ResultsTable}. Due to formatting limitation, our experimental results are the first and second sections, the latter are for reference.

We are unable to replicate \citet{1}'s main conclusion that RAKE is a superior alternative, by f-score and precision, when using a generated domain specific stop-list to TextRank. However we recognize that RAKE using a generic stoplist modestly outperforms an enhanced TextRank implementation by a small margin in all three measures. Also, we note that in the reference using a larger window size hampers the significantly performance of TextRank while in our experiments, it provided a minor increase in performance. We find that both methods have generally similar performance profile in precision, recall, and F-score. We recognize two caveats. One, we could not replicate the exact keyword generation process as there was insufficient details to do so. Two, we use an implementation of TextRank which includes minor enhancements, albeit this was the closest implementation found to the original. 

Overall, our experiments show that both RAKE and TextRank demonstrate similar levels of performance. By precision, both methods obtain values in the 25-30\% region and 40-45\% region by recall. Both methods suffer from generating too many false postive keywords/phrases although both methods have moderate abilities to detect relevant keywords/phrases. We are able to demonstrate similar levels of performance of RAKE using the Fox's stop-list and TextRank using a window size of 3 when comparing F-scores to the reference. RAKE using Fox' stop-list also demonstrates similar precision and recall. As the authors of both studies note, it is not possible to achieve perfect precision and recall upon this test data-set. Human annotated keywords may not necessarily be used in the abstract, hence they may not ever be produced via an extractive method.

Lastly, while runtimes were not a focal point for our analysis we could confirm that PyTextRank is slower than RAKE-NLTK via the profiler included in Spyder IDE. If these results are considered representative of RAKE and TextRank, then this confirms the claim in \citet{1} that RAKE has faster run-times. For pragmatic concerns, RAKE-NLTK is slightly preferable for its speed and marginally better performance.

\begin{table*}[t]
\captionsetup{justification=centering}
\caption{Results of automatic keyword extraction on 500 Inspec test abstracts using a Python implementation of RAKE \citep{1} and TextRank \citep{4}}	
\scalebox{1}{
\begin{tabularx}{\linewidth}{c c c c c c c c }
	\Xhline{2\arrayrulewidth} 
	                                         & \multicolumn{2}{c}{\makecell{Extracted \\ Keywords}} & \multicolumn{2}{c}{\makecell{Correct \\ Keywords}}       \\ \hline
	Method                                   &       Total        & Mean & Total            & Mean & Precision & Recall & F-Score \\ \hline
	\makecell[l]{Our RAKE Implementation}                  &  \\
	\makecell[l]{KA stoplist (generated)}  & 8724               & 17.4 & 1940             & 3.9  & 24.0       & 40.2    & 28.7          \\
	\makecell[l]{Fox stoplist}                             & 7969               & 15.9 & 2085             & \textbf{4.2}  & \textbf{27.3}       & \textbf{43.3}    & \textbf{32.1}          \\ \hline

	\makecell[l]{Our TextRank Implementation}              &  \\
	\makecell[l]{Undirected, co-occ. window = 2} & 7879         & 15.8 & 1972             & 3.9  & 25.9      & 41.5       & 30.8          \\
	\makecell[l]{Undirected, co-occ. window = 3} & 8192         & 16.4 & 2067             & 4.1 & 25.9    & 43.1       & 31.2          \\ \hline

	\small \makecell[l]{RAKE Ref. \citep{1} }			 &  \\ 
	\small\makecell[l]{KA stoplist (\textit{df} $>$ 10)}     		  & 6052 & 12.1 			& 2037 & 4.1  		& 33.7    & 41.5    & 37.2       \\
	\small \makecell[l]{Fox stoplist}     		 				 & 7893               & 15.8 & 2054             & 4.2  & 26         & 42.2    & 32.1      \\ \hline
	\normalsize	
	
	\small \makecell[l]{TextRank Ref. \citep{1} }			 &  \\ 
	\small \makecell[l]{Undirected, co-occ. window = 2} & 6784         & 13.6 & 2116             & 4.2  & 31.2      & 43.1       & 36.2          \\
	\small \makecell[l]{Undirected, co-occ. window = 3} & 6715         & 13.4 & 1897             & 3.8 & 28.2    & 38.6       & 32.6          \\ \hline
	\normalsize	
\end{tabularx}
\label{table:ResultsTable}
}
\end{table*} 

\subsection{Error Analysis}
This section elaborates upon three subjects. First, the general drop in performance of our experiments and the reference set. Second, our implementation of keyword adjacency stop-list generation and RAKE. Third, we compare the differences in output of RAKE and TextRank.

Comparing our experimental results and the reference, there are consistently more keywords extracted, but lower performance in the experimental results. One confounding factor is the pre-processing/reconstruction of abstracts and the tokenization of the abstracts in both methods. RAKE and TextRank rely upon assigning terms a score after tokenzing the text and processing from there after. It is likely some error was introduced in reconstructing the text from tokens in the XML format of the Hulth dataset and by variation by the tokenizers used by either implementation. There are three confounding sources of error here, the initial XML tokenized format, our reconstruction method, the tokenizer used in the implementations of RAKE and TextRank. Both RAKE and TextRank do not mention the exact tokenizer they used. We note that the mean correct keywords are fairly consistent. Since both methods output keywords based upon the size of their internal representation (eg. size of the TextRank graph), differences in tokenization will alter the amount of keywords/phrases returned. This may also explain the opposite in trends between the differing window sizes between our experiments and the reference of TextRank. A larger co-occurrence window can buffer against noisy tokenization. 

As mentioned in the implementation section we had difficulties replicating \citet{1}'s stop-list generation method. Following the description resulted in a stop-list of 99 words (13\%) of the referenced 763 list. For reference, the Fox stop-list is approximately 400 words. RAKE's authors try various degree-frequency thresholds for stop word candidacy in their keyword adjacency scheme, they list their findings in Table 1.4 of their paper. To summarize, they try three thresholds, decreasing in performance with stop-list size in parenthesis: 
\begin{enumerate}
	\small \itemsep0em 

	\item KA df = 10 (Size = 763, F-Score = 37.2)
	\item KA df = 25 (Size = 325, F-Score = 35.1)
	\item KA df = 35 (Size = 147, F-Score = 32.8).
	
\normalsize
\end{enumerate}  

Decreasing the size of the stop-list had considerable effects on the performance of RAKE. Our stop-list performs starts to perform similarly to the smaller KA stop-lists.  This contrast shows the importance of the stop-list in RAKE function and how error in our stop-list generation can confound the performance of RAKE. There appears to be marginally decreasing utility of larger stop-lists, however the marginal decrease is modest. Using a domain specific stop-list leads to better performance, contrasting the moderately sized Fox's stop-list (\char`\~400 terms) and the moderately size KA stop-list (325 terms) in the reference. 

Finally we compare the diversity of estimates generated via the two methods and note they may be complementary. RAKE's adjacency matrix is similar in intent to TextRank's graph representation and have similar performance profiles. We explore how different are the keywords generated by either method. We devise a simple measure of similarity as a size ratio of the intersection between two keyword sets generated divided by their union. If these methods are the same in essence, then we expect values relatively closer to 1. This would indicate that the intersection between keywords generated via two methods or variants constitutes the majority of terms returned by either method. We computed the following similarity scores:
\begin{enumerate}
	\small \itemsep0em 

	\item RAKE (Fox stoplist) and RAKE (KA stoplist): 0.5122
	\item TextRank (Window = 2) and TextRank (Window = 3):  0.6560
	\item RAKE (Fox stoplist) and TextRank (Window = 2): 0.2478
\normalsize
\end{enumerate}  

These similarity metrics indicate show that between methods, there is a greater level of similarity. RAKE's performance is highly dependent upon the stop-list used hence we see a greater variation (decreased similarity) between the Fox and KA stop-list methods. The window parameter to TextRank does not fundamentally alter the method as with RAKE's stoplists, it simply increases the distance two tokens are considered co-related, hence a relatively high similarity value is expected. It is interesting to see that  RAKE and TextRank have a low similarity scores indicating that their outputs may complement each other, since both score similarly in accuracy. We note that we investigated the keywords generated per abstract by both methods. In summary, the human-subjective similarity score is higher than that reported, though there are situations where the keywords generated complement each other. At times, TextRank generates slightly redundant keywords such as "firm growth rates" followed by the keyword "growth".  RAKE does not stem or lemmatize text hence it can result in redundant keywords as well such as "growth rate" and "growth rates" being returned. RAKE seems to return more key-phrases and longer key-phrases than TextRank. For example, RAKE returns "applied nonlinear time series analysis" while TextRank returned "nonlinear time series".

\section{Conclusion}

We find significant but not excessive differences in replication of \citet{1}'s evaluation, albeit we were unable to replicate their keyword adjacency scheme which is a significant factor in their findings. We are able to validate the performance of RAKE using Fox's stop-list and the results of TextRank to a sufficient extent. We find that RAKE performs similarly to TextRank using a generalized stop-list with a modest increase in performance in precision, recall, and f-score. Our replication notes difficulties that hinder public adoption of these two promising methods. First, RAKE's description of their complementary stop-list generation method hinders its flexibility. RAKE when bundled with a generic stop-list functions is comparable to its peer TextRank as an unsupervised method. RAKE using a domain specific stop-list outperforms its peer, however this method has difficulties to implement and moves the method away from being a truly unsupervised method. Second, many implementations of TextRank do not fully implement the method as described. Many implementations skip the post-processing of keywords into key-phrases which has considerable use. By our evaluation, both methods obtain values in the 25-30\% region by precision, 40-45\% region by recall and near 30\% by F-score. We hope our analysis can act as an anchor point for further analysis of these keyword extraction algorithms. 

Improvements to stop-list generation is a natural extension point for our work. We noted difficulties in replicating the complementary stop-list generation method supporting RAKE. Online, we find that few stop-list generation methods are available or even proposed. Developers prefer to bundle their methods with established generic stop-lists such as Fox's stop-list, NLTK's stop-list and Google Search's stop-list. Alternatively, some authors implement a simple top-frequency based stop-list. Few authors attempt to devise a method to construct domain specific stop-lists. We believe there is widespread applicability in this endeavor outside of RAKE as well.


\newpage
\section{Acknowledgments}

Teamwork Breakdown: 
\begin{enumerate}
	\small \itemsep0em 

	\item Shouyang Zhou:
	\begin{enumerate}
		\item Wrote interface to reconstruct abstracts from the pre-processed GitHub Repo source.
		\item Wrote the benchmark script to implemented and tabulate results of the experiment.
		\item Found implementations of RAKE and TextRank in Python.
		\item Code reviewed and tested implementations of RAKE and TextRank for a representative implementation.
		\item Analyzed final experimental results, investigated sources of experimental error.
		\item Primary author for sections: Implementation, Evaluation, Error analysis, Conclusion.

	\end{enumerate}

	\item Sharon Hains:
	\begin{enumerate}
		\item Wrote code for keyword adjacency stop-list generation scheme.
		\item Investigated difficulties with stop-list generation issues. Experimented with combining our generated stop-list and a partial stop-list given in the paper.
		\item Looked into publicly available stop-list generation methods with implementations. 
		\item Found the data-set used from \citet{hulth-2003-improved}
		\item Analyzed final experimental results, investigated sources of experimental error.
		\item Primary author for sections: Introduction, Related Work, Methodology
		\item Converted draft from word to Latex, maintained drafts of final report. Solver of latex formatting.
		\item Drafted Readme file.
	\end{enumerate}

	\item Sharif Bakouny:
	\begin{enumerate}
		\item Assisted with keyword adjacency stop-list generation implementation.
		\item Secondary author for sections: Related Work
		\item Summarized RAKE and TextRank papers.
		\item Primary author for Abstract.
	\end{enumerate}	

\normalsize
\end{enumerate}  
 

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\nocite{*}
\bibliography{report}
\bibliographystyle{acl_natbib}

%\appendix

%\section{Table 1.2 from \citet{1}}
%\begin{figure}[b!]
%  \centering
%  \includegraphics[width=\linewidth]{table1-2.jpg}
%  \caption{Table 1.2 from \citet{1}}
%\end{figure}


\end{document}
