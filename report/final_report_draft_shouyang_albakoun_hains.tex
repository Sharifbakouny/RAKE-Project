%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{array}
\usepackage{makecell}
\usepackage{url}
\usepackage{lipsum}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CMPUT 497 Project Report: \\ RAKE - Key Word Extraction Replication}

\author{Shouyang Zhou \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt shoyang@ualberta.ca} \\\And
  Sharon Hains \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt hains@ualberta.ca} \\\And
  Sharif Bakouny is the bst \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt albakoun@ualberta.ca} \\}
  


\date{}

\begin{document}
\maketitle

\section{Introduction}

We aim to replicate the main evaluation from the article "Automatic Keyword Extraction from Individual Documents" by \citet{1}. This paper devises an unsupervised method for keyword extraction titled "RAKE" and compares it to a previous well performing unsupervised method called "TextRank". Details to follow in the evaluation section. These two papers explore two unsupervised methods, RAKE based on word adjacency and TextRank based on generating an importance weighted word graph.

Keyword extraction is the automated process of extracting important words and phrases from a document. The importance of keyword extraction is in application, in information retrieval, feature engineering, and augmenting human classification tasks. At its most basic level, it helps humans process unstructured data in a more efficient and digestible manner. The importance of this replication study is to verify the results of \citet{1}, so we can be more confident in using RAKE as keyword extraction method. Confirming the results of RAKE testing is relevant as keyword extraction is a widely used application as discussed above. 

At its core, keyword/keyphrase extraction is generating a set of terms from a given section of natural unprocessed text. For our project, the input is the text and the output is the a series of generated keywords/keyphrases via "RAKE" and "TextRank". For each input text, keywords/keyphrases sets are generated via different "RAKE" and "TextRank" variants.    

We will be using the dataset used in RAKE as our input for evaluation, which is a set of abstracts, and processing the data. Our output will be a generation of keywords extracted from this set of abstracts. We will be comparing the number of correct keywords to extracted keywords, to determine how accurate our replication is to the original paper.

Additionally we try to replicate a stoplist generation method the authors suggest that complements RAKE. The input is abstract and human annotated keywords from \citep{hulth-2003-improved}. The output is a list of stopwords. This list will be used for our RAKE testing. In essence, the authors have suggested a supervised complementary stoplist generation scheme to RAKE. 

We will then be analyzing our output to create a table in the form of Table 1.2 in \citet{1}, please see the appendix for details. This will be a table comparing the performance of RAKE and TextRank variants (parameters) listing the metrics: extracted keywords (total, mean), correct keywords (total, mean), precision, recall, f-measure summarizing the replication-evaluation. 

\section{Related Work}

\citet{4} proposes TextRank, a graph-based ranking algorithm for keyword extraction, where the importance of a vertex (phrases) is decided by considering global information value of the phrase recursively computed from the entire graph (text). The authors of RAKE cite TextRank as the recent seminal work in unsupervised keyword/keyphrase generation. 

Implementing this algorithm goes as follows:

\begin{enumerate}
\item Select candidate text units and add them as vertices to a graph.
\item Generate edges of the graph via some relation between text units. (Authors suggest co-occurance)
\item Compute a recursive importance score of a vertex as some innate value plus the sum of its descendants scores.    
\item Iterate scoring until convergence via a damping factor. 
\item Rank vertices based on their final scores (values) and select top n vertices.
\item Reduce keywords into keyphrases by their co-occurance in the text.
\end{enumerate}   

RAKE and TextRank evaluate their method using the same methods and dataset from \citet{hulth-2003-improved}, an older seminal work in keyword/keyphrase extraction.

We will not be recreating the results of \citet{hulth-2003-improved}, however as we are including their results in our evaluation for reference due to missing training data. \citet{1} describe their method where "Hulth (2003) compares the effectiveness of three term selection approaches: noun-phrase (NP) chunks, n-grams, and POS tags, with four discriminative features of these terms as inputs for automatic keyword extraction using a supervised machine-learning algorithm." \citet{1} noted difficulty in finding training materials used by \citet{hulth-2003-improved}.    

\section{Methodology}

The original paper “Automatic Keyword Extraction from Individual Documents” by \citet{1} is the primary work of interest. As mentioned, they evaluate their method "RAKE" and compare it with "TextRank" over two datasets. RAKE requires a set of phrase and content-word delimiters called a stoplist, a list of stopwords. Stop words are punctuation, numbers, conjunctions, and user specified terms which are used to delimit candidate keyword/phrases. 

A brief summary of the RAKE algorithm:
\begin{enumerate}
	\item Split the text into an array of words using the word delimiters.
	\item Split the array into sequences contiguous words using stop words and phrase delimiters.
	\item Candidate keywords are words in a sequence that are assigned the same position in the text. 
	\item Assign scores to each keyword candidate using ratio of degree to word-frequency. 
	\item Keywords that contain stop words:
	\begin{enumerate}
		\item A pair of candidate keywords must be adjoined at least twice in the text in the same order.
		\item Create a new keyword which contains the pair of keywords with interior stopwords between them.
		\item The new keyword’s score is the sum of the scores of its keywords components.
	\end{enumerate}
	\item The keywords of the text are the top T keywords from the keyword candidates list.
\end{enumerate}

\citet{1} also develops methods for stoplist generation. These stoplist generation methods leverage supervised datasets to generate dataset specific (thus domain specific) stoplists. 

A brief summary of the stoplist generation algorithm:
\begin{enumerate}
	\item With a manually selected list of keywords, find each keyword in the abstract.
	\item Look to the left and right words of the abstract.
	\item Count these words as 'adjacent words'.
	\item Iterate through the adjacent words list.
	\item For each abstract, find the count of each adjacency word, called the adjacency frequency.
	\item For each keyword, find the count of each adjacency word, called the keyword frequency.
	\item If the keyword frequency is higher than the adjacency frequency, remove this word from the adjacent words list.
	\item All items remaining in the list is our stoplist. 
\end{enumerate}

Once we have generated the keyword adjacency stoplist, we will test it with our implementation of RAKE.

RAKE was compared to TextRank and seminal supervised learning methods \citep{hulth-2003-improved} over a dataset of human keyword annotated scientific paper abstracts originating from \citet{hulth-2003-improved}. As both RAKE and TextRank are unsupervised methods they evaluate both methods on the test set, they do not use the training dataset with one exception for RAKE. RAKE can use the training dataset for the purpose of generating a keyword adjacency stoplist, a domain specific stoplist. The performance of RAKE depends on the stoplist used. RAKE was found to outperform all previously used keyword extraction algorithms in precision, efficiency and simplicity when using a domain specific stoplist. Using a generic stoplist, RAKE was found to be no worse performing than TextRank.

\section{implementation}
Since our initial proposal, we have found implementations of RAKE and TextRank in python. Since these are already available to us, we will replicate \citeapos{1} evaluation of the two using third party libraries.  In essence, we will implement an evaluation script that feeds a dataset into these third party libraries to extract then aggregate the resultant metrics. This task involves data collection, understanding the interface to the RAKE and TextRank implementations, preprocessing datasets to be piped into the two methods, and extracting and aggregating the results.

The data from Hulth is sourced from \citet{Boudin}, this GitHub repo is hosted by an associate professor at the University of Nantes researching NLP and information retrieval. The dataset required a python interface to reproduce the inital unprocessed text and human annotated keywords. The file structure was quite odd. The abstracts were stored in XML where each abstract is decomposed into individual tokens-tags groups where each token had been post-proceesed with its POS tag, its stem, and various other qualities. Some effort was required to reconstruct the intital text.

We conducted a review of candidate third party implementations of RAKE and TextRank. The authors of RAKE and TextRank give unit examples of input and output which where used to establish functionality. In this review, we check that the implementation appears to follow the method descriptions as best to our ability. We focus on testing individual steps of the implementations on small examples and the author's examples. Special care was taken on examining the role of tuning parameters, some implementations did not expose an interface for the stated parameter tuning in the papers. In these cases, modifications were taken to expose these parameters.

We found that many public TextRank implementations did not expose the same parameters as in the paper and that some do not fully implement the method (\citet{TextRank-alt1}, \citet{TextRank-alt2}, \citet{TextRank-alt3}). These implementations generated significantly different keyword/keyphrases than that of the unit example. Furthermore, all three implementations did not implement the keyphrase post-processing as in the paper. TextRank authors suggest that if two keywords/keyphrases are adjacent in the text then they are collapsed into a single keyphrase. In effect, these proved to be only keyword extraction methods. 

We decided upon the "PyTextRank" (\citet{PyTextRank}), as our representative implementation of TextRank given its strong similarty to the author's unit examples and its inclusion of the keyphrase postprocessing. The authors of PyTextRank do note some modifications they make over the author's implementation. For example, PyTextRank leverages spaCy to lemmatize, chunk nouns, and conduct named entity recognition. These improvements serve to prune the graph generated in TextRank for better functionality. RAKE on the otherhand had fewer but common well functioning implementations (\citet{2}) that conformed to the examples given by the originiating paper. 

Lastly, we implementated the keyword adjacency stoplist generation method as described in \citet{1} and generated a stoplist from the Hulth training dataset. We found signifcant difficulties replicating the method described in RAKE, there were insufficent details to do so adequately. 

\section{Evaluation}

As our project is based on replicating the results of a paper, we evaluate the replication aggregate measures recorded as per the initial study and conduct an error analysis from samples from the replicating evaluation. 

\subsection{Data Sets \& Code Used}
For this reproduction we require three datasets:

\begin{enumerate}
\item \citet{hulth-2003-improved}'s dataset of human keyword annotated scientific paper abstracts. After postprocessing, this amounts to the test dataset-subset, and the human annotated keywords/keyphrases per abstract.
\item Fox's Stoplist, a generic stopword-list.
\item The top 100 keywords generated provided from \citeapos{1}. The full list of words used in \citeapos{1} keyword adjacency stoplist was not available, so we are using what was provided in the paper to compare against our generated stoplist. 
\end{enumerate}  

As described in the methodology section, we also used Python implementations of RAKE and TextRank. 

We will be generating the Keyword Adjacency stoplist, which will be reproduced using the algorithm described in \citet{hulth-2003-improved}. To complete this, we will be tokenizing both each abstract and its keywords with spaCy's PhraseMatcher, and then completing the stoplist generation algorithm with this tokenized text. We will then test RAKE with our generated stoplist.

\subsection{Evaluation Metrics}

The measures reported in the original study by method were: number of extracted keywords, correct number of extracted keywords, precision, recall, and f-measure (f-score).  We will compare our recorded measures to that of the original study to what extent are \citeapos{1} results reproducible. We will consider \citeapos{1} results reproducible if the ordinal performance between RAKE and TextRank variants can be verified. 

\subsection{Results}
to do 

\lipsum[3]
\begin{table*}[t]
\scalebox{0.9}{
\centering
\begin{tabular}{>{\raggedright\arraybackslash}p{2.9cm}c c c c c c c }
	\hline
	                                         & \multicolumn{2}{c}{\makecell{Extracted \\ Keywords}} & \multicolumn{2}{c}{\makecell{Correct \\ Keywords}}       \\ \hline
	Method                                   &       Total        & Mean & Total            & Mean & Precision & Recall & F-Measure \\ \hline
	RAKE                                     &  \\
	\makecell[l]{KA stoplist \\(generated)}  & 8891               & 17.8 & 1962             & 3.9  & 23.8       & 40.7    & 28.7          \\
	Fox stoplist                             & 8152               & 16.3 & 2125             & 4.3  & 27.2       & 44.3    & 32.4          \\ \hline
	\makecell[l]{\citep{1} \\ RAKE}			 &  \\ 
	\makecell[l]{KA stoplist \\(\textit{df} $>$ 10)}     		  & 6052 & 12.1 			& 2037 & 4.1  		& 33.7    & 41.5    & 37.2       \\
	Fox stoplist     		 				 & 7893               & 15.8 & 2054             & 4.2  & 26         & 42.2    & 32.1      \\ \hline
	TextRank                                 &  \\
	\makecell[l]{Undirected, co-occ. \\window = 2} &                    &      &                  &      &           &        &           \\
	\makecell[l]{Undirected, co-occ. \\window = 3} &                    &      &                  &      &           &        &           \\ \hline
\end{tabular}
}
\end{table*} 

\subsection{Error Analysis}
Fusce maximus quam nec odio ullamcorper, consectetur aliquam mauris malesuada. Sed lacinia elit lectus, sed rutrum augue dictum non. Pellentesque tempor felis non mi hendrerit, vel sagittis nibh ultrices. Proin ullamcorper lorem in orci sollicitudin imperdiet. Maecenas iaculis convallis facilisis. Nunc lobortis eleifend lectus non aliquet. Sed sagittis, mauris cursus imperdiet venenatis, mi purus tincidunt massa, vel gravida enim tellus id dui. Sed lobortis tincidunt turpis. Aenean rhoncus sapien ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus a rhoncus augue, quis sagittis nisi. Donec vulputate orci ut ultricies ornare. Vestibulum purus metus, aliquet in scelerisque vel, volutpat non sem. Donec hendrerit urna ante, non rutrum risus efficitur eget. Curabitur ligula felis, consectetur vitae ultricies id, condimentum et lectus. Morbi bibendum velit eget scelerisque dictum.

\section{Conclusion}

to do 

\section{Acknowledgments}

Teamwork Breakdown: Daniel wrote the code for re-creating the abstracts and the measures for tesing the output for TextRank and RAKE. Sharon wrote the code for generation the stoplist and found the datasets used from \citet{hulth-2003-improved}. Sharif worked with Sharon on the Keyword adjacency stoplist generation code and revised the final report. 
 

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{report}
\bibliographystyle{acl_natbib}

%\appendix

%\section{Table 1.2 from \citet{1}}
%\begin{figure}[b!]
%  \centering
%  \includegraphics[width=\linewidth]{table1-2.jpg}
%  \caption{Table 1.2 from \citet{1}}
%\end{figure}


\end{document}
