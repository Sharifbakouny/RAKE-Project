%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{array}
\usepackage{makecell}
\usepackage{url}
\usepackage{tabularx}
\usepackage{caption}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CMPUT 497 Project Report: \\ RAKE - Key Word Extraction Replication}

\author{Shouyang Zhou \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt shoyang@ualberta.ca} \\\And
  Sharon Hains \\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt hains@ualberta.ca} \\\And
  Sharif Bakouny\\
  University of Alberta \\
  Edmonton, Alberta, Canada \\
  {\tt albakoun@ualberta.ca} \\}
  


\date{}

\begin{document}
\maketitle

\section{Introduction}

We aim to replicate the main evaluation from the article "Automatic Keyword Extraction from Individual Documents" by \citet{1}. This paper devises an unsupervised method for keyword extraction titled "RAKE" and compares it to a previous well performing unsupervised method called "TextRank". Details to follow in the evaluation section. These two papers explore two unsupervised methods, RAKE based on word adjacency and TextRank based on generating an importance weighted word graph.

Keyword extraction is the automated process of extracting important words and phrases from a document. The importance of keyword extraction is in application, in information retrieval, feature engineering, and augmenting human classification tasks. At its most basic level, it helps humans process unstructured data in a more efficient and digestible manner. The importance of this replication study is to verify the results of \citet{1}, so we can be more confident in using RAKE as keyword extraction method. Confirming the results of RAKE testing is relevant as keyword extraction is a widely used application as discussed above. 

At its core, keyword/keyphrase extraction is generating a set of terms from a given section of natural unprocessed text. For our project, the input is the text and the output is the a series of generated keywords/keyphrases via "RAKE" and "TextRank". For each input text, keywords/keyphrases sets are generated via different "RAKE" and "TextRank" variants.    

We will be using the dataset used in RAKE as our input for evaluation, which is a set of abstracts, and processing the data. Our output will be a generation of keywords extracted from this set of abstracts. We will be comparing the number of correct keywords to extracted keywords, to determine how accurate our replication is to the original paper.

Additionally we try to replicate a stoplist generation method the authors suggest that complements RAKE. The input is abstract and human annotated keywords from \citep{hulth-2003-improved}. The output is a list of stopwords. This list will be used for our RAKE testing. In essence, the authors have suggested a supervised complementary stoplist generation scheme to RAKE. 

We will then be analyzing our output to create a table in the form of Table 1.2 in \citet{1}, please see the appendix for details. This will be a table comparing the performance of RAKE and TextRank variants (parameters) listing the metrics: extracted keywords (total, mean), correct keywords (total, mean), precision, recall, f-measure summarizing the replication-evaluation. 

\section{Related Work}

\citet{4} proposes TextRank, a graph-based ranking algorithm for keyword extraction, where the importance of a vertex (phrases) is decided by considering global information value of the phrase recursively computed from the entire graph (text). The authors of RAKE cite TextRank as the recent seminal work in unsupervised keyword/keyphrase generation. 

Implementing this algorithm goes as follows:

\begin{enumerate}
\item Select candidate text units and add them as vertices to a graph.
\item Generate edges of the graph via some relation between text units. (Authors suggest co-occurance)
\item Compute a recursive importance score of a vertex as some innate value plus the sum of its descendants scores.    
\item Iterate scoring until convergence via a damping factor. 
\item Rank vertices based on their final scores (values) and select top n vertices.
\item Reduce keywords into keyphrases by their co-occurance in the text.
\end{enumerate}   

RAKE and TextRank evaluate their method using the same methods and dataset from \citet{hulth-2003-improved}, an older seminal work in keyword/keyphrase extraction.

We will not be recreating the results of \citet{hulth-2003-improved}, however as we are including their results in our evaluation for reference due to missing training data. \citet{1} describe their method where "Hulth (2003) compares the effectiveness of three term selection approaches: noun-phrase (NP) chunks, n-grams, and POS tags, with four discriminative features of these terms as inputs for automatic keyword extraction using a supervised machine-learning algorithm." \citet{1} noted difficulty in finding training materials used by \citet{hulth-2003-improved}.    

\section{Methodology}

The original paper “Automatic Keyword Extraction from Individual Documents” by \citet{1} is the primary work of interest. As mentioned, they evaluate their method "RAKE" and compare it with "TextRank" over two datasets. RAKE requires a set of phrase and content-word delimiters called a stoplist, a list of stopwords. Stop words are punctuation, numbers, conjunctions, and user specified terms which are used to delimit candidate keyword/phrases. 

A brief summary of the RAKE algorithm:
\begin{enumerate}
	\item Split the text into an array of words using the word delimiters.
	\item Split the array into sequences contiguous words using stop words and phrase delimiters.
	\item Candidate keywords are words in a sequence that are assigned the same position in the text. 
	\item Assign scores to each keyword candidate using ratio of degree to word-frequency. 
	\item Keywords that contain stop words:
	\begin{enumerate}
		\item A pair of candidate keywords must be adjoined at least twice in the text in the same order.
		\item Create a new keyword which contains the pair of keywords with interior stopwords between them.
		\item The new keyword’s score is the sum of the scores of its keywords components.
	\end{enumerate}
	\item The keywords of the text are the top T keywords from the keyword candidates list.
\end{enumerate}

\citet{1} also develops methods for stoplist generation. These stoplist generation methods leverage supervised datasets to generate dataset specific (thus domain specific) stoplists. 

A brief summary of the stoplist generation algorithm:
\begin{enumerate}
	\item With a manually selected list of keywords, find each keyword in the abstract.
	\item Look to the left and right words of the abstract.
	\item Count these words as 'adjacent words'.
	\item Iterate through the adjacent words list.
	\item For each abstract, find the count of each adjacency word, called the adjacency frequency.
	\item For each keyword, find the count of each adjacency word, called the keyword frequency.
	\item If the keyword frequency is higher than the adjacency frequency, remove this word from the adjacent words list.
	\item All items remaining in the list is our stoplist. 
\end{enumerate}

Once we have generated the keyword adjacency stoplist, we will test it with our implementation of RAKE.

RAKE was compared to TextRank and seminal supervised learning methods \citep{hulth-2003-improved} over a dataset of human keyword annotated scientific paper abstracts originating from \citet{hulth-2003-improved}. As both RAKE and TextRank are unsupervised methods they evaluate both methods on the test set, they do not use the training dataset with one exception for RAKE. RAKE can use the training dataset for the purpose of generating a keyword adjacency stoplist, a domain specific stoplist. The performance of RAKE depends on the stoplist used. RAKE was found to outperform all previously used keyword extraction algorithms in precision, efficiency and simplicity when using a domain specific stoplist. Using a generic stoplist, RAKE was found to be no worse performing than TextRank.

\section{Implementation}
Since our initial proposal, we have found implementations of RAKE and TextRank in python. Since these are already available to us, we will replicate \citeapos{1} evaluation of the two using third party libraries.  In essence, we will implement an evaluation script that feeds a dataset into these third party libraries to extract then aggregate the resultant metrics. This task involves data collection, understanding the interface to the RAKE and TextRank implementations, preprocessing datasets to be piped into the two methods, and extracting and aggregating the results.

The data from Hulth is sourced from \citet{Boudin}, this GitHub repo is hosted by an associate professor at the University of Nantes researching NLP and information retrieval. The dataset required a python interface to reproduce the inital unprocessed text and human annotated keywords. The file structure was quite odd. The abstracts were stored in XML where each abstract is decomposed into individual tokens-tags groups where each token had been post-proceesed with its POS tag, its stem, and various other qualities. Some effort was required to reconstruct the intital text.

We conducted a review of candidate third party implementations of RAKE and TextRank. The authors of RAKE and TextRank give unit examples of input and output which where used to establish functionality. In this review, we check that the implementation appears to follow the method descriptions as best to our ability. We focus on testing individual steps of the implementations on small examples and the author's examples. Special care was taken on examining the role of tuning parameters, some implementations did not expose an interface for the stated parameter tuning in the papers. In these cases, modifications were taken to expose these parameters. Take for example the reviewal of PyTextRank. We verify that PyTextRank constructs an undirected graph which is used in the networkx's function PageRank, which is an information retrival method that is the basis of TextRank, and establish their tuning of the PageRank settings such as a convergence cofficent accordingly to the paper. We note that PyTextRank does not expose the window parameter used to establish word co-occurance relations, so we had to modify the library to expose this parameter.

We found that many public TextRank implementations did not expose the same parameters as in the paper and that some do not fully implement the method (\citet{TextRank-alt1}, \citet{TextRank-alt2}, \citet{TextRank-alt3}). These implementations generated significantly different keyword/keyphrases than that of the unit example. Furthermore, all three implementations did not implement the keyphrase post-processing as in the paper. TextRank authors suggest that if two keywords/keyphrases are adjacent in the text then they are collapsed into a single keyphrase. In effect, these proved to be only keyword extraction methods. 

We decided upon the "PyTextRank" (\citet{PyTextRank}), as our representative implementation of TextRank given its strong similarty to the author's unit examples and its inclusion of the keyphrase postprocessing. The authors of PyTextRank note minor enhancements they make over the author's implementation. PyTextRank leverages spaCy to lemmatize, chunk nouns, and conduct named entity recognition. These improvements serve to prune the graph generated in TextRank for better functionality. RAKE on the otherhand had fewer but common well functioning implementations (\citet{2}) that conformed to the examples given by the originiating paper. 

Lastly, we implemented the keyword adjacency stoplist generation method as described in \citet{1} and generated a stoplist from the Hulth training dataset. We found signifcant difficulties replicating the method described in RAKE, there were insufficent details to do so adequately.  

\section{Evaluation}

As our project is based on replicating the results of a paper, we evaluate the replication aggregate measures recorded as per the initial study and conduct an error analysis from samples from the replicating evaluation. 

This evaluation compares two variants of RAKE and two variants of TextRank. The authors use a generated stoplist and a generic stoplist when using RAKE and use differing parameters when using TextRank. RAKE requires a stoplist to remove function words which may have similar qualities to keywords as judged by their high prevalnce and connectivity. TextRank uses a window size parameter to establish co-occurance relations within the text. The window sizes compared are values of two and three, a window size of two indicates that the words must be adjacent to establish an relation/edge in the scoring graph. 

\subsection{Data Sets \& Code Used}
To summarize, we have gathered or recreated the following datasets:

\begin{enumerate}
\item \citet{hulth-2003-improved}'s dataset of human keyword annotated scientific paper abstracts. After postprocessing, this amounts to the test dataset-subset, and the human annotated keywords/keyphrases per abstract.
\item Fox's Stoplist, a generic stopword-list used in one RAKE.
\end{enumerate}  

We will be generating the Keyword Adjacency stoplist, which will be reproduced using the algorithm described in \citet{hulth-2003-improved}. To complete this, we will be tokenizing both each abstract and its keywords with spaCy's PhraseMatcher, and then completing the stoplist generation algorithm with this tokenized text. We will then test RAKE with our generated stoplist.

\subsection{Evaluation Metrics}

The measures reported in the original study by method were: number of extracted keywords, correct number of extracted keywords, average precision, average recall, and average f-measure (f1-score).  We will compare our recorded measures to that of the original study to what extent are \citeapos{1} results reproducible. We will consider \citeapos{1} results reproducible if the ordinal performance between RAKE and TextRank variants can be verified. 

\subsection{Results}

We present our replication below in the same format as \citet{1}. Due to formatting limitations, we will present the results from our experiment then the reference results section wise. Our experimental results are the first and third sections, the second and fourth sections reference the stated results from \citet{1}.

\begin{table*}[t]
\captionsetup{justification=centering}
\caption{Results of automatic keyword extraction on 500 Inspec test abstracts using a Python implementation of RAKE \citep{1} and TextRank \citep{4}}	
\scalebox{1}{
\begin{tabularx}{\linewidth}{c c c c c c c c }
	%\hline
	\Xhline{2\arrayrulewidth} 
	                                         & \multicolumn{2}{c}{\makecell{Extracted \\ Keywords}} & \multicolumn{2}{c}{\makecell{Correct \\ Keywords}}       \\ \hline
	Method                                   &       Total        & Mean & Total            & Mean & Precision & Recall & F-Score \\ \hline
	\makecell[l]{Our RAKE Implementation}                  &  \\
	\makecell[l]{KA stoplist \\(generated)}  & 8891               & 17.8 & 1962             & 3.9  & 23.8       & 40.7    & 28.7          \\
	\makecell[l]{Fox stoplist}                             & 8152               & 16.3 & 2125             & 4.3  & 27.2       & 44.3    & 32.4          \\ \hline

	\makecell[l]{\citep{1} \\ RAKE}			 &  \\ 
	\makecell[l]{KA stoplist \\(\textit{df} $>$ 10)}     		  & 6052 & 12.1 			& 2037 & 4.1  		& 33.7    & 41.5    & 37.2       \\
	\makecell[l]{Fox stoplist}     		 				 & 7893               & 15.8 & 2054             & 4.2  & 26         & 42.2    & 32.1      \\ \hline
	
	Our TextRank Implementation              &  \\
	\makecell[l]{Undirected, co-occ. \\window = 2} & 7884         & 15.8 & 1973             & 3.9  & 26      & 41.5       & 30.8          \\
	\makecell[l]{Undirected, co-occ. \\window = 3} & 8187         & 16.4 & 2064             & 4.13 & 25.9    & 43.0       & 31.2          \\ \hline

	\makecell[l]{\citep{1} \\ TextRank}			 &  \\ 
	\makecell[l]{Undirected, co-occ. \\window = 2} & 6784         & 13.6 & 2116             & 4.2  & 31.2      & 43.1       & 36.2          \\
	\makecell[l]{Undirected, co-occ. \\window = 3} & 6715         & 13.4 & 1897             & 3.8 & 28.2    & 38.6       & 32.6          \\ \hline
\end{tabularx}
}
\end{table*} 

We are unable to replicate \citet{1}'s main conclusion that RAKE is a superior alternative, by f-score and precision, when using a generated domain specific stoplist to TextRank. We find that both methods have similar performance profile in precision, recall, and F-score. We recognize two caveats. One, we could not replicate the exact keyword generation process as there was insufficent details to do so. Two, we use an implementation of TextRank which includes minor enhancements, albet this was the closest implementation found to the original. However we recognize that RAKE using a generic stoplist outperforms an enhanced TextRank implementation by a small margin in all three measures. Also, we note that in the reference using a larger window size hampers the significantly performance of TextRank while in our experiments, it provided a minor increase in performance.   

Overall, our experiments show that both RAKE and TextRank demonstrate similar levels of performance. By precision, both methods obtain values in the 25-30\% region and 40-45\% region by recall. Both methods suffer from generating too many false postive keywords/phrases although both methods have moderate abilities to detect relevant keywords/phrases. As the authors of both studies note, it is not possible to achieve perfect precision and recall upon this test dataset. Human annotated keywords/keyphrases may not necessarily be used in the abstract, hence they may not ever be produced via an extractive method.

Lastly, while runtimes were not a focal point for our analysis we could confirm that PyTextRank is slower than RAKE-NLTK via the profiler included in Spyder IDE. If these results are considered representative of RAKE and TextRank, then this confirms the claim in \citet{1} that RAKE has faster runtimes. For pragmatic concerns, RAKE-NLTK is slightly preferable for its speed and marginally better performance.

\subsection{Error Analysis}
Our error analysis will elaborate upon four subjects. Firstly, the overall drop in performance between our experiments and the reference set. Secondly, our implementation of the keyword adjacency stoplist generation method and its role in RAKE. Thirdly and lastly, we compare the differences in keywords extracted by RAKE and TextRank.

We note comparing our experimental results and the reference, there appears to be consistently more keywords extracted and lower performance in the experimental results. One confounding factor maybe the preprocessing of abstracts and the tokenization of the abstracts in both methods. RAKE and TextRank rely upon assigning terms a score based upon tokenzing the text and processing from there after. It is likely some error was introduced in reconstructing the text from tokens in the XML format of the Hulth dataset and by variation by the tokenizers used by either library. There are three confounding sources of error, the initial XML tokenized format, our reconstruction method, the tokenizer used in the implementations of RAKE and TextRank. Both RAKE and TextRank do not mention the exact tokenizer they used. We note that the mean correct keywords/keyphrases are fairly consistent. Since both methods output keywords/keyphrases based upon the size of their internal representation (eg. size of the TextRank graph), differences in tokenization will alter the amount of keywords/phrases returned.

Following \citet{1}'s stoplist generation algorithm resulted in a stoplist that was 13\% the size of \citet{1}'s 763 word list, with 99 words. We can see that is a significant deviation from \citet{1}'s results, and can infer a few reasons why these results may have occurred. As discussed previously, \citet{1} does not discuss how the keywords were matched in the abstract, only that they were matched and then looked to its adjacent words to continue on with the stoplist generation algorithm. We used spaCy's PhraseMatcher to find keyword matches in the abstracts, but on individual abstract evaluation, we found that many of the keywords had slightly different wording than what existed in the abstract. This rendered PhraseMatcher unable to find the keyword in the abstract. In addition, many of the keywords did not actually exist in the abstract. We can infer that during \citet{hulth-2003-improved}'s manual process of selecting keywords these abstracts, keywords were chosen that best represented the abstract subjects, not just existing words in the abstract.

TODO: Finish Section

\section{Conclusion}

-----TODO: need to add more -----

We have determined from the significant difference in results that to accurately replicate \citet{1}'s experiment, more details are required to generate a replicable keyword adjacency stoplist. This would include specifically with what method can the keywords be found in the abstracts. Further studies can be done to determine the best method for generating a stoplist based on keyword adjacency, where comparisons can be done using various string-matching methods.

\section{Acknowledgments}

Teamwork Breakdown: Daniel wrote the code for re-creating the abstracts and the measures for testing the output for TextRank and RAKE. Sharon wrote the code for generation the stoplist and found the datasets used from \citet{hulth-2003-improved}. Sharif worked with Sharon on the Keyword adjacency stoplist generation code and revised the final report. 
 

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{report}
\bibliographystyle{acl_natbib}

%\appendix

%\section{Table 1.2 from \citet{1}}
%\begin{figure}[b!]
%  \centering
%  \includegraphics[width=\linewidth]{table1-2.jpg}
%  \caption{Table 1.2 from \citet{1}}
%\end{figure}


\end{document}
